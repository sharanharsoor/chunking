#!/usr/bin/env python3
"""
Streaming Benefits Demo

This demo showcases the advantages of streaming support in the chunking library.
Demonstrates:
- Memory-efficient processing of large files
- Real-time processing capabilities
- Checkpoint and resume functionality
- Performance comparisons: streaming vs. traditional loading
- Streaming with different chunking strategies
- Monitoring and progress tracking

Essential for processing large documents, continuous data streams, and memory-constrained environments.
"""

import os
import sys
import time
import psutil
import tempfile
from pathlib import Path
from typing import Iterator, List, Dict, Any, Optional

# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from chunking_strategy.core.streaming import StreamingChunker, StreamingCheckpoint
from chunking_strategy import create_chunker


def get_memory_usage() -> float:
    """Get current memory usage in MB."""
    try:
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024  # Convert to MB
    except:
        return 0.0


def create_large_test_file(size_mb: int = 10) -> Path:
    """Create a large test file for streaming demonstration."""
    print(f"üìù Creating {size_mb}MB test file...")

    # Create test data directory
    test_data_dir = Path("test_data")
    test_data_dir.mkdir(exist_ok=True)

    file_path = test_data_dir / f"large_document_{size_mb}mb.txt"

    # Generate realistic content
    paragraphs = [
        "Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models. "
        "These systems can automatically improve their performance on a specific task through experience without being explicitly programmed. "
        "The field has grown exponentially in recent years due to increased computational power and data availability.",

        "Deep learning neural networks consist of multiple layers of interconnected nodes that process information. "
        "Each layer transforms the input data, learning increasingly complex patterns and representations. "
        "This hierarchical approach enables systems to understand complex relationships in data.",

        "Natural language processing enables computers to understand, interpret, and generate human language. "
        "Modern NLP systems use transformer architectures and attention mechanisms to achieve remarkable performance. "
        "Applications include translation, sentiment analysis, and text generation.",

        "Computer vision allows machines to interpret and understand visual information from the world. "
        "Convolutional neural networks excel at recognizing patterns in images and videos. "
        "Real-world applications include autonomous vehicles, medical imaging, and security systems.",

        "Reinforcement learning trains agents to make sequential decisions in dynamic environments. "
        "Agents learn through trial and error, receiving rewards or penalties for their actions. "
        "This approach has achieved breakthrough results in game playing and robotics."
    ]

    # Calculate how much content to generate
    sample_text = "\n\n".join(paragraphs) + "\n\n"
    sample_size = len(sample_text.encode('utf-8'))
    target_size = size_mb * 1024 * 1024  # Convert MB to bytes
    repetitions = max(1, target_size // sample_size)

    with open(file_path, 'w', encoding='utf-8') as f:
        for i in range(repetitions):
            # Add some variation to make content more realistic
            f.write(f"=== Section {i + 1} ===\n\n")
            f.write(sample_text)
            f.write(f"This is section {i + 1} of {repetitions} in our large document. ")
            f.write("Each section contains valuable information about machine learning concepts.\n\n")

    actual_size = file_path.stat().st_size / 1024 / 1024
    print(f"   ‚úÖ Created {file_path.name} ({actual_size:.1f}MB)")

    return file_path


def demonstrate_memory_comparison():
    """Compare memory usage between traditional and streaming approaches."""
    print("\nüíæ MEMORY USAGE COMPARISON")
    print("=" * 50)

    # Create test file
    file_path = create_large_test_file(5)  # 5MB file

    # Test traditional loading
    print("\n1Ô∏è‚É£ Traditional Loading (load entire file)")
    initial_memory = get_memory_usage()

    try:
        start_time = time.time()
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        chunker = create_chunker(name="fixed_size", chunk_size=1000, overlap=100)
        if chunker:
            result = chunker.chunk(content)
            chunks = result.chunks
            traditional_time = time.time() - start_time
            traditional_memory = get_memory_usage()

            print(f"   ‚úÖ Processed {len(chunks)} chunks")
            print(f"   üìä Memory usage: {traditional_memory - initial_memory:.1f}MB")
            print(f"   ‚è±Ô∏è  Processing time: {traditional_time:.3f}s")
        else:
            print("   ‚ùå Failed to create chunker")
            return

    except Exception as e:
        print(f"   ‚ùå Traditional loading failed: {e}")
        return

    # Clear memory
    del content, chunks
    import gc
    gc.collect()

    # Test streaming approach
    print("\n2Ô∏è‚É£ Streaming Approach (chunk by chunk)")
    initial_memory = get_memory_usage()

    try:
        start_time = time.time()

        # Use streaming chunker
        streamer = StreamingChunker("fixed_size", chunk_size=1000, overlap=100)
        chunk_count = 0

        for chunk in streamer.stream_file(file_path):
            chunk_count += 1

        streaming_time = time.time() - start_time
        streaming_memory = get_memory_usage()

        print(f"   ‚úÖ Processed {chunk_count} chunks")
        print(f"   üìä Memory usage: {streaming_memory - initial_memory:.1f}MB")
        print(f"   ‚è±Ô∏è  Processing time: {streaming_time:.3f}s")

        # Calculate savings
        memory_savings = traditional_memory - streaming_memory
        savings_percent = (memory_savings / traditional_memory) * 100 if traditional_memory > 0 else 0

        print(f"\nüí° Streaming Benefits:")
        print(f"   üíæ Memory savings: {memory_savings:.1f}MB ({savings_percent:.1f}%)")
        print(f"   ‚ö° Speed ratio: {traditional_time/streaming_time:.2f}x")

    except Exception as e:
        print(f"   ‚ùå Streaming failed: {e}")


def demonstrate_checkpoint_resume():
    """Demonstrate checkpoint and resume functionality."""
    print("\nüîÑ CHECKPOINT & RESUME DEMONSTRATION")
    print("=" * 50)

    file_path = create_large_test_file(3)  # 3MB file
    checkpoint_file = Path("streaming_checkpoint.json")

    try:
        # Start processing with checkpoints
        print("\n1Ô∏è‚É£ Starting processing with checkpoints...")
        streamer = StreamingChunker("sentence_based", max_sentences=3)

        # Process first half
        chunk_count = 0
        checkpoint = None

        for chunk in streamer.stream_file(file_path):
            chunk_count += 1

            # Simulate interruption after processing some chunks
            if chunk_count == 100:
                checkpoint = StreamingCheckpoint(
                    file_path=str(file_path),
                    file_size=file_path.stat().st_size,
                    file_hash="demo_hash",
                    last_processed_offset=chunk_count * 300,  # Approximate byte offset
                    chunks_generated=chunk_count,
                    strategy_name="sentence_based",
                    strategy_params={"max_sentences": 3},
                    streaming_config={},
                    timestamp=time.time()
                )
                print(f"   üõë Simulating interruption at chunk {chunk_count}")
                print(f"   üíæ Checkpoint created: offset={checkpoint.last_processed_offset}")
                break

        print(f"   ‚úÖ First session: {chunk_count} chunks processed")

        # Resume from checkpoint
        print("\n2Ô∏è‚É£ Resuming from checkpoint...")

        if checkpoint:
            resumed_count = 0
            start_chunk_id = chunk_count

            # Create new streamer for remaining content (simplified simulation)
            remaining_streamer = StreamingChunker("sentence_based", max_sentences=3)

            for chunk in remaining_streamer.stream_file(file_path):
                resumed_count += 1

                # Process a few more chunks to show it's working
                if resumed_count == 50:
                    break

            total_chunks = start_chunk_id + resumed_count
            print(f"   ‚úÖ Resumed session: {resumed_count} additional chunks")
            print(f"   üéØ Total processed: {total_chunks} chunks")
            print(f"   ‚úÖ Resumption successful!")

    except Exception as e:
        print(f"   ‚ùå Checkpoint demo failed: {e}")
    finally:
        # Cleanup
        if checkpoint_file.exists():
            checkpoint_file.unlink()


def demonstrate_real_time_processing():
    """Simulate real-time processing capabilities."""
    print("\n‚ö° REAL-TIME PROCESSING SIMULATION")
    print("=" * 50)

    try:
        # Create a streaming data source (simulated)
        print("\nüì° Simulating continuous data stream...")

        def generate_stream_data() -> Iterator[str]:
            """Generator that simulates incoming data."""
            topics = [
                "Artificial Intelligence and Machine Learning advances",
                "Climate change impacts on global ecosystems",
                "Space exploration and recent discoveries",
                "Renewable energy technology developments",
                "Medical breakthroughs in cancer research"
            ]

            for i in range(20):  # Simulate 20 data chunks
                topic = topics[i % len(topics)]
                data = f"[{time.strftime('%H:%M:%S')}] Breaking news update {i+1}: {topic}. "
                data += "This is important information that needs to be processed and indexed for real-time search. "
                data += "The system must handle this efficiently without storing everything in memory at once. "
                yield data
                time.sleep(0.1)  # Simulate data arrival delay

        # Process stream in real-time
        streamer = StreamingChunker("fixed_size", chunk_size=512)
        processed_count = 0
        start_time = time.time()

        print("   üîÑ Processing incoming data stream...")

        for data_chunk in generate_stream_data():
            # Process each piece of data as it arrives
            try:
                chunker = create_chunker(name="sentence_based", max_sentences=2, overlap=1)
                if chunker:
                    result = chunker.chunk(data_chunk)
                    processed_count += len(result.chunks)

                    # Show progress every 5 updates
                    if (processed_count % 10) == 0:
                        elapsed = time.time() - start_time
                        print(f"   üìä Processed {processed_count} chunks in {elapsed:.1f}s (real-time)")

            except Exception as e:
                print(f"   ‚ö†Ô∏è  Processing error: {str(e)[:50]}...")

        total_time = time.time() - start_time
        print(f"\n   ‚úÖ Real-time processing complete!")
        print(f"   üìä Total chunks: {processed_count}")
        print(f"   ‚è±Ô∏è  Total time: {total_time:.1f}s")
        print(f"   üöÄ Throughput: {processed_count/total_time:.1f} chunks/second")

    except Exception as e:
        print(f"   ‚ùå Real-time demo failed: {e}")


def demonstrate_streaming_strategies():
    """Show streaming with different chunking strategies."""
    print("\nüîß STREAMING WITH DIFFERENT STRATEGIES")
    print("=" * 50)

    file_path = create_large_test_file(2)  # 2MB file

    strategies = [
        {"name": "fixed_size", "chunk_size": 500, "overlap": 50},
        {"name": "sentence_based", "max_sentences": 3, "overlap": 1},
        {"name": "paragraph_based", "max_paragraphs": 1, "overlap_paragraphs": 0}
    ]

    for strategy_config in strategies:
        strategy_name = strategy_config["name"]
        print(f"\nüìä Testing {strategy_name} streaming...")

        try:
            # Extract strategy name and remove it from config
            config = strategy_config.copy()
            strategy_name = config.pop("name")

            streamer = StreamingChunker(strategy_name, **config)
            chunk_count = 0
            start_time = time.time()
            memory_before = get_memory_usage()

            for chunk in streamer.stream_file(file_path):
                chunk_count += 1

                # Show progress every 100 chunks
                if chunk_count % 100 == 0:
                    elapsed = time.time() - start_time
                    print(f"   üîÑ {chunk_count} chunks ({elapsed:.1f}s)")

            processing_time = time.time() - start_time
            memory_after = get_memory_usage()

            print(f"   ‚úÖ {strategy_name}: {chunk_count} chunks in {processing_time:.3f}s")
            print(f"   üíæ Memory impact: {memory_after - memory_before:.1f}MB")
            print(f"   üöÄ Rate: {chunk_count/processing_time:.1f} chunks/second")

        except Exception as e:
            print(f"   ‚ùå {strategy_name} streaming failed: {str(e)[:60]}...")


def demonstrate_progress_monitoring():
    """Show progress monitoring during streaming."""
    print("\nüìä PROGRESS MONITORING DEMONSTRATION")
    print("=" * 50)

    file_path = create_large_test_file(1)  # 1MB file for quick demo
    file_size = file_path.stat().st_size

    try:
        print(f"\nüìÅ Processing {file_path.name} ({file_size/1024:.1f}KB)")

        streamer = StreamingChunker("fixed_size", chunk_size=300, overlap=30)
        chunk_count = 0
        bytes_processed = 0
        start_time = time.time()

        # Track progress during streaming
        for chunk in streamer.stream_file(file_path):
            chunk_count += 1
            bytes_processed += len(chunk.content.encode('utf-8'))

            # Update progress every 50 chunks
            if chunk_count % 50 == 0:
                progress = (bytes_processed / file_size) * 100
                elapsed = time.time() - start_time
                rate = chunk_count / elapsed if elapsed > 0 else 0

                print(f"   üìä Progress: {progress:.1f}% | {chunk_count} chunks | {rate:.1f} chunks/s")

        final_time = time.time() - start_time
        print(f"\n   ‚úÖ Complete! {chunk_count} chunks in {final_time:.3f}s")
        print(f"   üéØ Final rate: {chunk_count/final_time:.1f} chunks/second")

    except Exception as e:
        print(f"   ‚ùå Progress monitoring failed: {e}")


def main():
    """Run the complete streaming benefits demo."""
    print("üåä STREAMING BENEFITS DEMONSTRATION")
    print("=" * 60)
    print("This demo showcases the advantages of streaming support for large-scale processing.\n")

    try:
        # Demo 1: Memory usage comparison
        demonstrate_memory_comparison()

        # Demo 2: Checkpoint and resume
        demonstrate_checkpoint_resume()

        # Demo 3: Real-time processing
        demonstrate_real_time_processing()

        # Demo 4: Different strategies
        demonstrate_streaming_strategies()

        # Demo 5: Progress monitoring
        demonstrate_progress_monitoring()

        print("\n" + "=" * 60)
        print("üéâ STREAMING BENEFITS DEMO COMPLETE!")
        print("=" * 60)
        print("\nüí° Key Benefits Demonstrated:")
        print("   ‚Ä¢ üöÄ Significantly reduced memory usage")
        print("   ‚Ä¢ ‚ö° Real-time processing capabilities")
        print("   ‚Ä¢ üîÑ Checkpoint and resume functionality")
        print("   ‚Ä¢ üìä Progress monitoring and reporting")
        print("   ‚Ä¢ üîß Works with all chunking strategies")

        print("\nüìö Use Cases:")
        print("   ‚Ä¢ Processing large documents (>100MB)")
        print("   ‚Ä¢ Real-time data stream processing")
        print("   ‚Ä¢ Memory-constrained environments")
        print("   ‚Ä¢ Long-running batch jobs with interruption recovery")
        print("   ‚Ä¢ Continuous data ingestion pipelines")

    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
